---
title: "5. Model Training and Tuning"
author: "Russ Conte"
date: '2022-05-22'
output: html_document
---

# Model Training and Tuning

Contents

[5.1 Model Training and Parameter Tuning](#Model Training and Parameter Tuning)<br>
[5.2 An Example](#5.2 An Example)<br>
[5.3 Basic Parameter Tuning](#5.3 Basic Parameter Tuning)<br>
[5.4 Notes on Reproducibility](#5.4 Notes on Reproducibility)<br>
[5.5 Customizing the Tuning Process](#5.5 Customizing the Tuning Process)<br>
[5.5.1 Pre-Processing Options](#5.5.1 Pre-Processing Option)<br>
[5.5.2 Alternate Tuning Grids](#Alternate Tuning Grids)<br>
[5.5.3 Plotting the Resampling Profile](#Plotting the Resampling Profile)<br>
[5.5.4 The `trainControl` Function](#5.5.4 The `trainControl` Function)<br>
[5.5.5 Alternate Performance Metrics](#Alkternate Performance Metrics)<br>
[5.6 Choosing the Final Model](#Choosing the Final Model)<br>
[5.7 Extracting Predictions and Class Probabilities](#Extracting Predictions and Class Probabilities)<br>
[5.8 Exploring and Comparing Resampling Distrubutions](#Exploring and Comparing Resampling Distributions)<br>
[5.8.1 Within-Model](#5.8.1 Within-Model)<br>
[5.8.2 Between-Models](#Between-Models)<br>
[5.9 Fitting Models Without Parameter Tuning](#5.9 Fitting Models Without Parameter Tuning)<br>


### <a id="5.1 Model Training and Parameter Tuning"></a>5.1 Model Training and Paramter Tuning###

The `caret` package has several functions that attempt to streamline the model building and evaluaton process.

The `train` function can be used to:
* Evaluate, using resampling, the effect of model tuning parameters on model performance
* Choose the "optimal" model across these parameters
* Estimate model performance from a training set

First, a specific model must be chosen. Current, 238 are available using `caret`.

The first step in tuning the model (line 1 in the algorithm below) is to choose a set of parameters to evaluate. For example, if fitting a Partial Least Squares (PLS) model, the number of PLS components to evaluate must be specified:

1. Define sets of model parameter values to evaluate
2. **for** *each parameter set* **do**
3.    **for** *each resampling iteration* **do**<br>
    4. Hold-out Specific Items<br>
    5. [Optional] Pre-Process the data<br>
    6. Fit the model on the remainder<br>
    7. Predict the Hold-out samples<br>
8.    **end**
9.    Calculate the average performanc across hold-out predictions
10. **end**
11. Determine the optimal parameter set
12. Fit the final model to all the training data using the optimal parameter set.


Once the model and tuning parameter values have been defined, the type of resampling should also be specified. Currently, *k*-fold cross-validation(once or repeated), leave-one-out cross-validation and bootstrap (simple estimation or the 632 rule) resampling methods can be used by `train`. After resampling, the process produces a profile of performance measure available to guide the user as to which tuning parameter values should be chosen. By default, the function augomatically chooses the tuning parameters associationed with the best values, although different algorithms can be used (see details below)


### <a id="5.2. An Example"></a>5.2. An Example###

The Sonar data are available in the `mlbench` package. Here, we load the data:

```{r An example, using the Sonar data}

library(mlbench)
data(Sonar)
str(Sonar[,1:10])

```

The function `createDataPartition` can be used to create a stratified random sample of data into training and test sets:

```{r Create stratified random samples of the data into training and test sets}

library(caret)
set.seed(998)
inTraining <- createDataPartition(Sonar$Class, p = 0.75, list = FALSE)
training <- Sonar[inTraining,]
testing <- Sonar[-inTraining,]

training # 157 observations of 61 variables
testing # 51 observations of 61 variables

```


### <a id="5.3 Basic Parameter Tuning"></a>5.3 Basic Parameter Tuning###



### <a id="5.4 Notes on Reproducibility"></a>5.4 Notes on Reproducibility###



### <a id="5.5 Customizing the Tuning Process"></a>5.5 Customizing the Tuning Process###


### <a id="5.5.1 Pre-Processing Options"></a>5.5.1 Pre-Processing Options###



### <a id="5.5.2 Alternate Tuning Grids"></a>5.5.2 Alternate Tuning Grids###


### <a id="5.5.3 Plotting the Resampling Profile"></a>5.5.3 Plotting the Resampling Profile###


### <a id="5.5.4 The `trainControl` Function"></a>5.5.4 The `trainControl` Function###


### <a id="5.5.5 Alternate Performance Metrics"></a>5.5.5 Alternate Performance Metrics###


### <a id="5.6 Choosing the Final Model"></a>5.6 Choosing the Final Model###


### <a id="5.7 Extracting Predictions and Class Probabilities"></a>5.7 Extracting Predictions and Class Probabilities###



### <a id="5.8.2 Between-Models"></a>5.8.2 Between-Models###



### <a id="5.9 Fitting Models Without Parameter Tuning"></a>Fitting Models Without Parameter Tuning###


