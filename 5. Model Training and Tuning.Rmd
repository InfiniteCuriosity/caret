---
title: "5. Model Training and Tuning"
author: "Russ Conte"
date: '2022-05-22'
output: html_document
---

# Model Training and Tuning

Contents

[5.1 Model Training and Parameter Tuning](#Model Training and Parameter Tuning)<br>
[5.2 An Example](#5.2 An Example)<br>
[5.3 Basic Parameter Tuning](#5.3 Basic Parameter Tuning)<br>
[5.4 Notes on Reproducibility](#5.4 Notes on Reproducibility)<br>
[5.5 Customizing the Tuning Process](#5.5 Customizing the Tuning Process)<br>
[5.5.1 Pre-Processing Options](#5.5.1 Pre-Processing Option)<br>
[5.5.2 Alternate Tuning Grids](#Alternate Tuning Grids)<br>
[5.5.3 Plotting the Resampling Profile](#Plotting the Resampling Profile)<br>
[5.5.4 The `trainControl` Function](#5.5.4 The `trainControl` Function)<br>
[5.5.5 Alternate Performance Metrics](#Alkternate Performance Metrics)<br>
[5.6 Choosing the Final Model](#Choosing the Final Model)<br>
[5.7 Extracting Predictions and Class Probabilities](#Extracting Predictions and Class Probabilities)<br>
[5.8 Exploring and Comparing Resampling Distrubutions](#Exploring and Comparing Resampling Distributions)<br>
[5.8.1 Within-Model](#5.8.1 Within-Model)<br>
[5.8.2 Between-Models](#Between-Models)<br>
[5.9 Fitting Models Without Parameter Tuning](#5.9 Fitting Models Without Parameter Tuning)<br>


### <a id="5.1 Model Training and Parameter Tuning"></a>5.1 Model Training and Paramter Tuning###

The `caret` package has several functions that attempt to streamline the model building and evaluaton process.

The `train` function can be used to:
* Evaluate, using resampling, the effect of model tuning parameters on model performance
* Choose the "optimal" model across these parameters
* Estimate model performance from a training set

First, a specific model must be chosen. Current, 238 are available using `caret`.

The first step in tuning the model (line 1 in the algorithm below) is to choose a set of parameters to evaluate. For example, if fitting a Partial Least Squares (PLS) model, the number of PLS components to evaluate must be specified:

1. Define sets of model parameter values to evaluate
2. **for** *each parameter set* **do**
3.    **for** *each resampling iteration* **do**<br>
    4. Hold-out Specific Items<br>
    5. [Optional] Pre-Process the data<br>
    6. Fit the model on the remainder<br>
    7. Predict the Hold-out samples<br>
8.    **end**
9.    Calculate the average performanc across hold-out predictions
10. **end**
11. Determine the optimal parameter set
12. Fit the final model to all the training data using the optimal parameter set.


Once the model and tuning parameter values have been defined, the type of resampling should also be specified. Currently, *k*-fold cross-validation(once or repeated), leave-one-out cross-validation and bootstrap (simple estimation or the 632 rule) resampling methods can be used by `train`. After resampling, the process produces a profile of performance measure available to guide the user as to which tuning parameter values should be chosen. By default, the function augomatically chooses the tuning parameters associationed with the best values, although different algorithms can be used (see details below)


### <a id="5.2. An Example"></a>5.2. An Example###

The Sonar data are available in the `mlbench` package. Here, we load the data:

```{r An example, using the Sonar data}

library(mlbench)
data(Sonar)
str(Sonar[,1:10])

```

The function `createDataPartition` can be used to create a stratified random sample of data into training and test sets:

```{r Create stratified random samples of the data into training and test sets}

library(caret)
set.seed(998)
inTraining <- createDataPartition(Sonar$Class, p = 0.75, list = FALSE)
training <- Sonar[inTraining,]
testing <- Sonar[-inTraining,]

training[1:10, 1:10] # 157 observations of 61 variables
testing[1:10, 1:10] # 51 observations of 61 variables

```


### <a id="5.3 Basic Parameter Tuning"></a>5.3 Basic Parameter Tuning###

By default, simple bootstrap resampling is used for line 3 in the algorithm above. Other methods are available, such as *K*-fold cross-validation, leave-one-out, etc. The function, `trainControl` can be used to specify the type of resampling:

```{r Using trainControl to set up resampling}

fitControl <- trainControl(## 10-fold cross-validation,
                            method = "repeatedcv",
                            number = 10,
                            ## repeated ten times,
                            repeats = 10)

```


More information about `trainControl` is given in a section below.

The first two arguments to `train` are the predictor and outcome data objects, respectively. The third argument, `method`, specifies the type of model. To illustrate, we will fit a boosted tree model via the `gbm` package. The basic syntax for fitting this model using repeated cross-validation is shown below:

```{r Fitting a boosted tree using gbm}

set.seed(825)
gbmfit1 <- train(Class~., data = training,
                 method = "gbm",
                 trControl = fitControl,
                 ## This last option is actually one for gbm() that passes through
                 verbose = FALSE)
gbmfit1

```

For a gradient boosting machine (GBM) model, there are three (?) main tuning parameters:

* number of iterations, i.e. trees, (called `n.trees` in the `gbm` function)
* complexity of the tree, called `interaction.depth`
* learning rate: how quickly the algorithm adapts, called `shrinkage`
* the minimum number of training set samples in a note to commence splitting(`n.minobsinnode`)

The default values tested for this model are shown in the first two columns (`shrinkage` and `n.minobsinnode` are not shown because the grid set of candidate models all use a single value for these tuning parameters). The column labeled `accuracy` is the overall agreement rate averaged over cross-validation iterations. The agreement standard deviation is also calculated from the cross-validation results. The column, `Kappa` is Cohen's (unweighted) Kappa statistic averaged across the resampling results. `train` works with specific models. For these models, `train` can automatically create a grid of tuning parameters. By default, if *p* is the number of tuning parameters, the grid size is 3^p. As another example, regularized discriminant analysis (RDA) models have two parameters (`gamma` and `lambda`), both of which lie between zero and one. The default trainng grid would produce nine combinations in this two-dimensional space.

### <a id="5.4 Notes on Reproducibility"></a>5.4 Notes on Reproducibility###

Many models utilize random numbers during the phase where parameters are estimated. Also, the resampling indices are chosen using random numbers. There are two main ways to control the randomness in order to assure reproducible results:

* There are two approaches to ensuring that the same *resamples* are used between calls to `train`. The first is to use `set.seed` just prior to calling `train`. The first use of random numbers is to create the resampling information. Alternatively, if you would like to use specific splits of the data, the `index` argument of the `trainControl` function can be used. This is briefly discussed below.
* When the models are created *inside of resampling*, the seeds can also be set. While setting the seed prior to calling `train` may guarantee that the same rendom numbers are used, this is unlikely to be the case when parallel processing is used (depending on which technology is utilized). To set the model fitting seeds, `trainControl` has an additional argument called `seeds` that can be used. The value for this argument is a list of integer vectors that are used as seeds. The help oage for `trainControl` describes the appropriate format for this option.


### <a id="5.5 Customizing the Tuning Process"></a>5.5 Customizing the Tuning Process###

There are a few ways to customize the process of selecting tuning/complexity parameters and building the final model.

#### <a id="5.5.1 Pre-Processing Options"></a>5.5.1 Pre-Processing Options###

As previously mentioned, `train` can pre-process the data in various ways prior to model fitting. The function `preProcess` is automatically used. The function can be used for centering and scaling imputation (see details below) applying the spatial sign transformation and feature extraction via principal component analysis or independent component analysis.

To specify what pre-processing should occur, the `train` function has an argument called `preProcess`. This argument takes a character string of methods that would normally be passed to the `method` argument of the `preProcess` function. Additional options to the `preProcess` function can be passed via the `trainControl` function.

These processing steps would be applied during any predictions generated using `predict.train`, `extractPrediction` or `extractProbs` (see details later in this document). The pre-processing would **not** be applied to predictions that directly use the `object$finalModel` object.

For imputation, there are three methods currently implemented:

* *k*-nearest neighbors takes a sample with missing values and finds the *k* closest samples in the training set. The average of the *k* training set values for that predictor are used as a substitute for the original data. When calculating the distrances to the training set samples, the predictors used in the calculation are the ones with no missing values for that sample and no missing values in the training set.
* Another approach is to fit a bagged tree model for each predictor using the training set samples. This is usually a fairly accurate model and can handle missing values. When a predictor for a sample requires imputation, the values for the other predcitors are fed through the bagged tree and the prediction is used as the new value. The model van have significant computational cost.
* The median of the predictor's training set, PCA and ICA models only use complete samples.


#### <a id="5.5.2 Alternate Tuning Grids"></a>5.5.2 Alternate Tuning Grids###

The tuning parameter grid can be specified by the user. The argument `tuneGrid` can take a data frame with columns for each tuning parameter. The column names should be the same as the fitting function's arguments. For the previously mentioned RDA a example, the names would be `gamma` and `lambda`. `train` will tune the model over each combination of values in the rows

For the boosted tree model, we can fix the learning rate and evaluate more than three values of `n.trees`:

```{r gbm model}

gbmGrid <- expand.grid(interaction.depth = c(1,5,9),
                       n.trees = (1:30) * 50,
                       shrinkage = 0.1,
                       n.minobsinnode = 20)

nrow(gbmGrid)

set.seed(825)

gbmFit2 <- train(Class ~ ., data = training,
                 method = "gbm",
                 trControl = fitControl,
                 verbose = FALSE,
                 ## Now specify the exact models to evaluate:
                 tuneGrid = gbmGrid)

gbmFit2
```


#### <a id="5.5.3 Plotting the Resampling Profile"></a>5.5.3 Plotting the Resampling Profile###

The `plot` function can be used to examine the relationship between estimates of performance and the tuning parameters. For example, a simple invocation of the function shows the results for the first performance measure::

```{r Plotting the resampling profile}

trellis.par.set(caretTheme())
plot(gbmFit2)

```

Other performance metrics can be shown using the `metric` option:

```{r Other performance metrics}

trellis.par.set(caretTheme())
plot(gbmFit2, metric = "Kappa")

```

Other types of plots are also available. See `?plot.train` for more details. The code below shows a heatmap of the results:

```{r Heatmap of the results}

trellis.par.set(caretTheme())
plot(gbmFit2, metric = "Kappa", plotType = "level", scales = list(x = list(rot = 90)))

```

A `ggplot` method can also be used:

```{r using ggplot to plot the results}

ggplot(gbmFit2)

```

There are also plot functions that show more detailed representations of the resample the estimates. See `?xyplot.train` for more details.

From these plants, a different set of tuning parameters may be desired. To change the final values without starting the whole process again, the ` update that train ` can be used to refer to the final model see `?update.train`


#### <a id="5.5.4 The `trainControl` Function"></a>5.5.4 The `trainControl` Function###

The function `trainControl` generates parameters that further control how models are created, with possible values :

* `method`: The sampling method : `"boot"` , `"cv"`, `"LOOCV"`, `"LGOCV`, `"repeatedcv`, `"timeslice"`, `"none"`, and `"oob"`.The last value, out of bag estimates, can only be used by random forest, bagged trees, bagged earth, bagged flexible discriminant analysis, or conditional tree models. GBM models are not included. Also, for leave-one-out cross validation, no uncertainty estimates are given for the resampled performances measures.

* `number` and `repeats`: `number` controls with the number of folds in *K*-fold cross validation or number of resampling iterations for bootstrapping and leave group-out-cross validation. `repeats` applied only to repeated K fold cross validation. Suppose that `method = "repeatedcv`, `number = 10` and `repeats = 3`, then three separate 10-fold corss-validations are used as the resampling scheme.

* `verboseIter`: A logical for printing a training log.

* `returnData`: A logical for saving the data into a slot called `trainingData`.

* `p`: For leave-group-out cross-validation: The training percentage

* for `method = "timeslice"`, `trainControl` has options `initialWindow`, `horizon`. and `fixedWindow` that govern how cross-validation can be used for time series data.

* `classProbs`: A logical value determining whether class probabilities should be computed for held-out samples during resample.

* `index` and `indexOut`: Optional lists with elements for each reasmpling iteration. Each list element is the sample rows used for training at that iteratin or should be held-out. When these valures are not specified, `train` will generate them.

* `summaryFunction`: A function to compute alternate performance summaries.

* `selectionFunction`: A function to choose the optimal tuning parameters, and examples.

* `PCAthresh`, `ICAcomp`, and `k`: These are all options to pass to the `preProcess` function (when used)

* `returnResamp`: A character string containing one of the following values: `"all"`, `"final"`, or `"none"`. This specifies how much of the resampled performance measures to save.

* `allowParallel`: A logical that governs whether `train` should use parallel processing (if available).

There are several other options not discussed here.


#### <a id="5.5.5 Alternate Performance Metrics"></a>5.5.5 Alternate Performance Metrics###

The user can change the metric used to determine the best settings. By default, RMSE, $R^2$, and the mean absolute error (MAE) are computed for regression while accuracy and Kappa are computed for classification. Also by default, the parameter values are chosen using RMSE and accuracy, respectively for regression and classification. The `metric` argument of the `train` function allows the user to control which of the optimality criterion is used. For example, in problems where there is a low percentage of samples in one class, using `metric = "Kappa"` can improve the quality of the final model.

If none of these parameters are satisfactory, the user can also compute custom performance metrics. The `trainControl` function as an argument called `summaryFunction` especially as a function for computing performance the function for computing performance. The function should have these arguments:

* ` Data ` as a reference for a data frame or matrix with columns called ` OBS ` ` PRAD ` for the observed and predicted outcome values (either numeric data for regression or character values for classification). Currently, class probabilities are not passed to the function. The values in data or the held out predictions (and the associated reference values) for a single combination of tuning parameters. If they ` class problemRight ` aAdditionally if the ` recipe ` method for ` train ` was used, other variables that used in the model will also be included. This can be accomplished by adding a role in the recipe of ` "performance bar" `. And example is given in the recipe section of the site.
* `lev` is a character string tht has the outcome factor levels taken from the training data. For regression, a value of `NULL` is passed into the function.
* `model` is a character string for the model being used (i.e. the value passed to the `method` argument of `train`).

The output to the function should be a vector of numeric summary metrics with non-null names. By default, `train` evaluates classification models in terms of the predicted classes. Optionally, class probabilities can also be used to measure performance. To obtain predicted class probabilities within the resampling process, the argument `classProbs` and `trainControl` must be set to `back to back back`TRUE`. This merges columns of probabilities the predictions generated from each resample (there is a column per class and the column names are the class names).

As shown in the last section, custom functions can be used to calculate performance scores that are averaged over the resamples. Another builg-in function, `twoClassSummary`, will compute the sensitivity, specificity and area under the ROC curve:

```{r twoClassSummary}

head(twoClassSummary)

```

To rebuild the boosted tree model using this criterion, we can see the relationship between the tuning parameters and the area under the ROC curve using the following code:

```{r Rebuild the boosted tree model}

fitControl <- trainControl(method = "repeatedcv",
                           number = 10,
                           repeats = 10,
                           ## Estimates class probabilities
                           classProbs = TRUE,
                           ## Evaluate performance using the following function
                           summaryFunction = twoClassSummary)

set.seed(825)
gbmfit3 <- train(Class ~ ., data = training,
                 method = "gbm",
                 trControl = fitControl,
                 verbose = FALSE,
                 tuneGrid = gbmGrid,
                 # Specify which metric to optimize
                 metric = "ROC")

gbmfit3$bestTune # The optimal model

which(gbmfit3$results$ROC >= 0.9213)

gbmfit3$results[20,]

```


### <a id="5.6 Choosing the Final Model"></a>5.6 Choosing the Final Model###

Another method for customizing the tuning process is to modify the algorithm that is used to select the "best" parameter values, given the performance numbers. By default, the ` train ` function chooses the model with the largest performance value (or smallest, Fermin squared error in regression modelsOther schemes for selecting models can be used Bremen at all (1984) suggested The "one standard error rule" for simple tree-based models. In this case, the model with the best performance value is identified and, using resampling, we can estimate the standard error of performance. The final model used was the simplest model within one standard error of the (empirically) best model. With simple trees this makes sense, since these models will start to overfit as they become more and more specific to the training data.

`train` allows the user to specify alternate rules for selecting the final model. The argument `select function` can be used to supply a function to algorithmically determine the final model. There are three existing functions in the package: `best` chooses the largest/smallest value, `oneSE` attempts to capture the spirit of Breiman et at (1984) tand `tolerance` selects the least complex model within some percent tolerance of the best value. See `?best` for more details.

User-defined functions can be used, as long as they have the following arguments:

* `x` is a data frame containing the tune parameters and their associated performance metrics. Each row corresponds to a different tuning parameter combination.
* `metric` is a character string indicating which performance metric should be optimized (this is passed in directly from the `metric` argument of `train`).
* `maximize` is a single logical value indicating whether larger values of the performance metric are better (this is also directly passed from the call to `train`).

The function should output a single integer indicating which row in `x` is chosen.

As an example, if we choose the previous boosted tree model on the basis of overall accuracy, we would choose: end. Trees equals 1450, interaction dot depth equals five, shrinkage equals zero. One, and that men abs in node equals 20. However, the scale in this plot is fairly tight, with accuracy value arranging from zero. 86320. 922.Less complex model (e.g. if you are, Michelle are trees right princess might also yelled acceptable accuracy.

The tolerance function can be used to find a less complex model based on $\frac{x - x_{best}}{x_{best} x 100}$, which is the percent difference. For example, to select parameter values based on a 2% loss of performance:

```{r Finding the best model within a 2% loss of performance}

whichTwoPct <- tolerance(gbmfit3$results, metric = "ROC",
                         tol = 2, maximize = TRUE)
cat("best model within 2 pct of best:\n")

```

```{r Finding the best model:}

gbmfit3$results[whichTwoPct, 1:6]

```

This indicates that we can get a less complex model with an area under the curve ROC of 0.914 (compared to the "pick the best" value of 0.922).

The main issue with these functions is related to ordering the models from simplest to complex. In some cases this is easy(e.g. Simple trees, partial least squares. But in case of such this model, the ordering of models is subjective. For example, is a boosted three model using 100 iterations and a three depth of two more complex than one with 50 iterations in a three depth of eight? The package makes some choices regarding these order. In the case of boosted trees, the package assumes that increasing the number of iterations adds complexity at a faster rate than increasing the tree depth, some models are ordered on the number of iterations then ordered on depth. See `?best` for more examples for specific models.

### <a id="5.7 Extracting Predictions and Class Probabilities"></a>5.7 Extracting Predictions and Class Probabilities###

As previously mentioned, objects produced by the ` train ` function contain the "optimize" model in the ` final model ` sub-object. Predictions can be made from these objects as usual. In some cases, such as ` PLS ` or ` GBM ` objects, additional parameters from the optimized fit may need to be specified.In these cases, the ` train ` objects uses the results of the perimeter optimization to predict new samples. For example, if predictions were created using ` predict that GPM `, do use would have to specify the number of trees directly (there is no default).Also, for binary classification, the predictions from this function take the form of the probability of one of the classes, so extra steps are required to convert this to a factor factor. ` predict that train ` automatically handles these details for this (and for other mod

Also for `predict.train`, type options are standardized to be `"class"` and `"prob"` (the underlying code matches these to the appropriate choices for each model. For example:

```{r Extracting predictions}

predict(gbmfit3, newdata = head(testing))

```

```{r Extracting predictions part 2}

predict(gbmfit3, newdata = head(testing), type = "prob")

```

```{r}

gbmfit3
?Sonar
```


### <a id="5.8 Exploring and Comparing Resampling Distributions"></a>5.8 Exploring and Comparing Resampling Distributions###


#### <a id="5.8.1 Within-Model></a>5.8.1 Within-Model####

There are several `lattice` functions that can be used to explore relationships between tuning parameters and the resampling results for a specific model:

* `xyplot` and `stripplog` can be used to plot resampling statistics against (numeric) tuning parameters.
* `histogram` and `densityplot` can also be used to look at distributions of the tuning parameters across tuning parameters.

For example, the following statements create a density plot:

```{r Density plot of the tuning parameters}

trellis.par.set(caretTheme())
densityplot(gbmfit3, pch = "|")

?resamples

```

Note that if you are interested in putting the resampling results across multiple tuning parameters, the option `resamples equals "all"` should be used in the control object.


### <a id="5.8.2 Between-Models"></a>5.8.2 Between-Models###

The `caret` package also includes functions to characterize the differences between models (generated using `train`, `SBF` or `RFE`) via their resampling distributions. These functions are based on the work of Hothorn et al. (2005) and Eugster at al. (2008).

First, a support vector machine model is fit to the sewn our data. The data are centered and scaled using the `preProc` argument. Note that the same random number seed is that prior to the model that is identical to the seed used for the boosted tree model. This ensures that the same resampling sets are used, which will come in handy when we compare the resampling profiles between models.

```{r Using a SVM model}

set.seed(825)
svmFit <- train(Class ~ ., data = training,
                method = "svmRadial",
                trControl = fitControl,
                preProc = c("center", "scale"),
                tuneLength = 8,
                metric = "ROC")

svmFit

```

Also, a regularized discriminant analysis model was fit.

```{r Regularized discriminant analysis model}

set.seed(825)
rdaFit <- train(Class ~ ., data = training,
                method = "rda",
                trControl = fitControl,
                tuneLength = 4,
                metric = "ROC")

rdaFit

```

Given these models, can we make statistical statements about their performancd differences? To do this, we first collect the resampling results using `resamples`.

```{r Collect resampling results}

resamps <- resamples(list(GBM = gbmfit3,
                      SVM = svmFit,
                      RDA = rdaFit))

resamps

```

```{r Summary resamples}

summary(resamps)

```

Note that, in this case, the option `resamples = "final"` should be user-defined in the control objects.

There are several lattice plot methods that can ge used to visualize the resampling distributions: Density plots, box-whister plots, scatterplot matrices and scatter plots of summary statistics. For example:

```{r Plots of summary statistics}

theme1 <- trellis.par.get()
theme1$plot.symbol$col = rgb(.2, .2, .2, .4)
theme1$plot.symbol$pch = 16
theme1$plot.line$col = rgb(1, 0, 0, .7)
theme1$plot.lines$lwd <-  2
trellis.par.set(theme1)
bwplot(resamps, layout = c(3,1))

```

```{r Dotplot of resample data}

trellis.par.set(caretTheme())
dotplot(resamps, metric = "ROC")

```

```{r Scatter plot of resample data}

trellis.par.set(theme1)
xyplot(resamps, what = "BlandAltman")

```

```{r Scatterplot matrix}

splom(resamps)

```

Other visualizations are available in ` density plot that resembles ` and ` parallel.resembles `

Since models are fit on the same versions of the training data, it makes sense to make inferences on the differences between models. In this way we reduce the within a reasonable correlation that may exist. We can compute the differences, then use a simple TN test to evaluate the null hypothesis that there is no difference between the models.

```{r Calculate differences}

difValues <- diff(resamps)
difValues

```

```{r Summary of difValues}

summary(difValues)

```

```{r Box-whisker plot of difvalues}

trellis.par.set(theme1)
bwplot(difValues, layout = c(3,1))

```

```{r dot plot of difference values}

trellis.par.set(caretTheme())
dotplot(difValues)

```

### <a id="5.9 Fitting Models Without Parameter Tuning"></a>Fitting Models Without Parameter Tuning###

In cases where the model tuning values are known, `train` can be used to fit the model to the entire training set without any resampling or parameter tuning. Using the `method = "none"` option in `trainControl` can be used. For example:

```{r Fitting models without parameter tuning}

fitControl <- trainControl(method = "none", classProbs = TRUE)

set.seed(825)

gbmFit4 <- train(Class ~ ., data = training,
                 method = "gbm",
                 trControl = fitControl,
                 verbose = FALSE,
                 # Only a single model can be passed to the function when no resampling is used:
                 tuneGrid = data.frame(interaction.depth = 4,
                                       n.trees = 100,
                                       shrinkage = 0.1,
                                       n.minobsinnode = 20),
                 metric = "ROC")
gbmFit4

```

Note that `plot.train`, `resamples`, `confusionMatris.train` and several other functions will not work with this object, but `predict.train` and others will:

```{r Predict without parameter tuning}

predict(gbmFit4, newdata = head(testing))

```

```{r Predicting probabilities of metal or rock}

predict(gbmFit4, newdata = head(testing), type = "prob")

```

