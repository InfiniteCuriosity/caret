---
title: "3. Pre-Processing using caret"
author: "Russ Conte"
date: "4/30/2022"
output: html_document
---

`caret` includes several functions to pre-process the predictor data. It assumes that all of the data are numeric (i.e. factors have been converted to dummy variables via `model.matrix`, `dummyVars`, or other means.)

### 3.1 Creating Dummy Variables

The function `dummuVars` can be used to generate a complete (less than full rank parameterized) set of dummy variables from one or more factors. The function takes a formula and a data set and outputs an object that can be used to create the dummy variables using the predict method.

For example, the `etitanic` data set in the <span style="color:blue">`earth`</span> package includes two factors: `pclass` (passenger class, with levels 1st, 2nd, and 3rd) and `sex` (with levels female, male).

Let's load all the packages in two lines of code:

```{r}
Packages = c("AppliedPredictiveModeling", "caret", "tidyverse", "e1071", "earth")
lapply(Packages, library, character.only = TRUE)
```

Using `dummyVars`:

```{r Using dummyVars to create dummy variables in the titanic data set}

dummies <- dummyVars(survived~., data = etitanic)
head(predict(dummies, newdata = etitanic))

```

The authors point out there is no intercept, and each factor has a dummy variable for each level. This may not be useful for some modeling situations, such as `lm`.

Same idea, using the diamonds data set from tidyverse

```{r Creating dummy variables using the Boston Housing data set}

dummies <- dummyVars(price~., data = diamonds)
head(predict(dummies, newdata = diamonds))

```


### 3.2 Zero and Near Zero Variance Predictors

In some situations, the data generating mechanism can create predictors that only have a single unique value (i.e. a "zero-variance predictor"). For many models (excluding tree-based models) this may cause the model to crash or the fit to be unstable.

In a similar way, predictors might have only a handful of unique values gthat occur with very low frequencies. An example is the mdrr data set, and the `nR11` descriptor (number of 11-membered rings):

```{r}

data(mdrr)
data.frame(table(mdrrDescr$nR11))
```

The concern here is that these predictors may become zero – variance predictors when the data are split into cross – validation/bootstrap sub – summaries or that a few samples may have an undo influence on the model. These "nearest – zero – variance "predictors may need to be identified and eliminated prior to modeling.

To identify these types of predictors, the following two metrics can be calculated:

- The frequency of the most prevalent value over the second most frequent value left process (called the "frequency ratio"), which would be near one for while behaved predictors and very large for highly balanced data and
- The "percentage of unique values" is the Number of unique value divided by the total number of samples parentheses times 100) zero as a granularity of the data increases.

If the frequency ratio was greater than a pre-– specified threshold and the unique value percentage is less than a threshold, we might consider a predictor to be near zero variance.

We would not want to false the identify data that have low granurity but are evenly distributed, such a from a discreet uniform distribution. Using both criteria should not false detect such predictors.

Looking at the MDRR data, the `nearZeroVar` function can be used to identify near zero-variance variables (the `saveMetrics` argument can be used to show the details and usually defauls to `FALSE`)


```{r Applying the `nearZeroVar` function to identify near-zero-variance variables}

nzv <- nearZeroVar(mdrrDescr, saveMetrics = TRUE)
nzv[nzv$nzv,][1:10,]

```

```{r}

nzv <- nearZeroVar(mdrrDescr)
filteredDescr <- mdrrDescr[, -nzv]
dim(filteredDescr)

```

By default `nearZeroVar` will return the positions of the variables that are flagged to be problems.

### 3.3 Identifying Correlated Predictors

Given a correlation matrix, the `findCorrelation` function uses the following algorithm to flag predictors for removal:

```{r Identify and remove correlated predictors}

descrCor <- cor(filteredDescr)
highCorr <- sum(abs(descrCor[upper.tri(descrCor)]) > 0.999)

```

For the previous MDRR data, there are 65 descriptors that are almost perfectly correlated (correlation greater than 0.99), such as the total information index of composition (`IAC`) and the total information contact index (neighborhood symmetry of zero-order) (`TIC0`) (correlation equals one). The code chunk below shows the effect of removing the descriptors with absolute correlations above 0.75

```{r Remove features correlated above 0.75}

descrCor <- cor(filteredDescr)
summary(descrCor[upper.tri(descrCor)])

```

```{r}

highlyCorDescr <- findCorrelation(descrCor, cutoff = 0.75)
filteredDescr <- filteredDescr[,-highlyCorDescr]
descrCor2 <- cor(filteredDescr)
summary(descrCor2[upper.tri(descrCor2)])

```

## Linear Dependencies

The function `findlinearCombos` uses the QR decomposition of a matrix to enumerate sets of linear combinations (if they exist). For example, consider the following metrics that could have been produced by less than full rank parameterization of a two-way experimental layout:

```{r Linear Dependencies 1}

ltfrDesign <- matrix(0, nrow=6, ncol=6)
ltfrDesign[,1] <- c(1, 1, 1, 1, 1, 1)
ltfrDesign[,2] <- c(1, 1, 1, 0, 0, 0)
ltfrDesign[,3] <- c(0, 0, 0, 1, 1, 1)
ltfrDesign[,4] <- c(1, 0, 0, 1, 0, 0)
ltfrDesign[,5] <- c(0, 1, 0, 0, 1, 0)
ltfrDesign[,6] <- c(0, 0, 1, 0, 0, 1)

```

Note that columns two and three add up to the first column. Similarly, columns four, five and six and up to the first column. `findlinearCombos` will return a list that enumerates these dependencies. for each linear combination, it will incrementally remove columns from the matrix and test to see if the dependencies have been resolved. `findlinearCombos` will also return of vector of column positions which can be removed to eliminate linear dependencies:

```{r Remove lineard dependencies 2}

comboInfo <- findLinearCombos(ltfrDesign)
comboInfo

```

```{r Remove linear dependencies}

ltfrDesign[,-comboInfo$remove]

```

These types of dependencies can arise when large numbers of binary chemical fingerprints are used to descrivbe the structure of a molecule.

## 3.5 The `preProcess` Function

The `preProcess` class can be used for many operations on predictors, including centering and scaling. The function `preProcess` estimates the required parameters for each operation and `predict.preProcess` is used to apply them to specific data sets. This function can also be interfaced when calling the `train` function.

Several types of techniques are described in the next few sections and then another example is used to demonstrate how multiple methods can be used. Note that, in all cases, the `preprocess` function estimates whatever it requires from a specific data set (e.g. the training set) and applies these transformations to *any* data set without recomputing the values.

## 3.6 Centering and Scaling

In the example below, the half of the MDRR data are used to estimate the location and scale of the predictors. The function `preProcess` doesn't actually pre-process the data. `predict.preProcess` is used to pre-process this and other data sets.

```{r pre-process the MDRR data}

set.seed(96)
inTrain <- sample(seq(along = mdrrClass), length(mdrrClass)/2)

training <- filteredDescr[inTrain,]
test <- filteredDescr[-inTrain,]
trainMDRR <- mdrrClass[inTrain]
testMDRR <- mdrrClass[-inTrain]

preProcValues <- preProcess(training, method = c("center", "scale"))

trainTransformed <- predict(preProcValues, training)
testTransformed <- predict(preProcValues, test)

```

Let's look at the resuls:

```{r Look at the results of Centering and Scaling the MDRR data set}

mdrrClass

inTrain

training

test

trainMDRR

testMDRR

preProcValues

trainTransformed

testTransformed
```

Let's do the same with the MPG data set from Tidyverse

```{r Center and scale with the MPG data set from tidyverse}

mpg <- mpg %>% 
  mutate(id = row_number())

# training set:

train1 <- mpg %>% 
  sample_frac(size = 0.5)

test1 <- mpg[-train1$id,]

preProcValue1 <- preProcess(x = train1, method = c("center", "scale"))

train1Transformed <- predict(preProcValue1, train1)
test1Transformed <- predict(preProcValue1, test1)

train1Transformed
test1Transformed
```

## 3.7 Imputation

`preProcess` can be used to impute data sets based only on information in the training set. One method of doing this is with K-nearest neighbors. For an arbitrary sample, the K closest neighbors are found in the training set and the value of the predictor is imputed using these values (e. g. Using the mean).Using this approach will automatically trigger `pre-process` to center and scale the data, regardless of what is in the `method` argument. Alternatively, bagged trees can also be used to impute. For each predictor in the data, a bagged tree is created using all of the other predictors in the training set. When a new sample has a missing predictor value, the bagged model is used to predict the value. While, in theory, this is a more powerful method of imputing, the computational costs are much higher than the nearest neighbor technique.