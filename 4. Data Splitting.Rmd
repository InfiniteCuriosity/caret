---
title: "4. Data Splitting"
author: "Russ Conte"
date: '2022-05-21'
output: html_document
---

# Data Splitting
<h5>
[4.1 Simple Splitting Based on the Outcome](#4.1 Simple Splitting Based on the Outcome)<br>
[4.2 Splitting Based on the Predictors](#4.2 Splitting Based on the Predictors)<br>
[4.3 Data Splitting for Time Series](#4.3 Data Splitting for Time Series)<br>
[4.4 Data Splitting with Important Groups](#4.4 Data Splitting with Important Groups)<br>
</h5>

### <a id="4.1 Smiple Splitting Based on the Outcome"></a>4.1 Simple Splitting Based on the Outcome###

The function `createDataPartition` can be used to create balanced splits of the data. If the `y` argument of this function is a factor, the random sampling occurs with each class and should preserve the overall class distrubution of the data. For example, o create a single 80/20% split of the iris data:

```{r Creating an 80/20 split of the iris data}
library(caret)
trainIndex <- createDataPartition(iris$Species, p = 0.8,
                                  list = FALSE,
                                  times = 1)
head(trainIndex)
```

```{r Setting up test/train splits based on the 80/20 split of the data}

irisTrain <- iris[trainIndex,]
irisTest <- iris[trainIndex,]

head(irisTest)
head(irisTest)

```

The `list = FALSE` avoids returning the data as a list. This function also has an argument, `times`, that can create multiple splits at once; the data indices are returned in a list of integer vecrors. Similarly, `createResample` can be used to make simple bootstrap samples and `createFolds` can be used to generate balanced cross = validation groupings from a set of data.

```{r Create a 75/25 train/test split in the Diamonds data set}

trainIndex <- createDataPartition(diamonds$cut, p = 0.75,
                                  list = FALSE,
                                  times = 1)
head(trainIndex)

```


```{r View the data}

diamondsTrain <- diamonds[trainIndex,]
diamondsTest <- diamonds[-trainIndex,]
head(diamondsTrain)
head(diamondsTest)

```



### <a id="4.2 Splitting Based on the Predictors"></a>4.2 Splitting Based on the Predictors###
Also, the function `maxDissim` can be used to create sub-samples using a maximum dissimilarity approach. (Willett, 1999). Suppose there is a data set *A* with *m* samples and a larger data set *B* with *n* samples. We may want to create a sub-sample from *B* that is diverse when compared to *A*. To do this, for each sample in *B*, the function calculated the *m* dissimilarities between each point in *A*. The most dissimilar point in *B* is added to *A* and the process continues. There are many methods in R to calculate dissimilarity. `caret` uses the `proxy` package. See the manual for that package for a list of available measures. Also, there are many ways to calculate which sample is "most dissimilar". The argument `obj` can be used to specify any function that returns a scalar measure. `caret` includes two functions, `minDiss` and `sumDiss` that can be used to maximize the minimum and total dissimilarities, respectfully.

As an example, the figure below shows a scatter plot of two chemical descriptors for the Cox2 data. Using an initial random sample of 5 compounds, we can select 20 more compounds from the data so that the new compounds are most dissimilar from the initial 5 that were specified. The panels in the figure show the results using several combinations of distance metrics and scoring functions. For these data, the distance measure has less of an impact than the scoring method for determining which compounds are most dissimilar.

```{r Splitting based on predictors}

library(mlbench)
data("BostonHousing")

testing <- scale(BostonHousing[,c("age", "nox")])
set.seed(5)
## A random sample of five data points
startSet <- sample(1:dim(testing)[1],5)
samplePool <- testing[-startSet,]
start <- testing[startSet,]
newSamp <- maxDissim(start, samplePool, n = 20)
head(newSamp)

```

```{r Finding the most dissimilar predictors in the diamonds data set}
library(tidyverse)
library(caret)
testing <- diamonds[,c("price", "carat")]
set.seed(314)
## A random sample of five data points
startSet <- sample(1:dim(testing)[1],1000)
samplePool <- testing[-startSet,]
start <- testing[startSet,]
newSample <- maxDissim(start, samplePool, n = 10)
newSample

?maxDissim

```


### <a id="4.3 Data Splitting for Time Series"></a>4.3. Data Splitting for Time Series###

Simple random sampling of time series is probably not the best way to resample time series data. Hyndman and Athanasopoulos (2013) discuss *rolling forecasting origin* techniques that move the training and test sets in time. `caret` contains a function called `createTimeSlices` that can create the indices for this type of splitting.

The three parameters for this type of splitting are:

* `initialWindow`: the initial number of consecutive values in each training set sample.
* `horizon`: The number of consecutive values in test set sample
* `fixedWindow`: A logical: if `FALSE`, the training set always starts at the first sample and the training set size will vary over data splits.

As an example, suppose we have a time series with 20 data points. We can fix `initialWidnow-5` and look at different settings of the other two arguments. In the plot below, rows in each panel correaspond to different data splits(i.e. resamples) and the columns correspond to different data points. Also, red indicates samples that are included in the training set and blue indicates samples in the test set.


### <a id="4.4 Data Splitting with Important Groups"></a>4.4 Data Splitting with Important Groups###

In some cases there is an important qualitative factor in the data that should be considered during (re)sampling. For example:
* in clinical trials, there may be hospital-to-hospital differences
* with longitudinal or repeated measures data, subjects (or general independent experimental unit) may have multiples rows in the data set, etc.

There may be an interest in making sure that these groups are not contained in the training and testing set since this may bias the test set performance to be more optimistic. Also, when one or more specific groups are held out, the resampling might capture the "ruggedness" of the model. In the example where clinical data is recorded over multiple sites, the resampling performance estimates partly measure how extensible the model is across sites.

To split the data based on groups, `groupKFold` can be used:

```{r split data based on groups}

set.seed(3527)
subjects <- sample(1:20, size = 80, replace = TRUE)
table(subjects)

```

```{r Calculate and show folds of the data}

folds <- groupKFold(subjects, k= 15)
head(folds)
length(folds) # a list with ten elements

```

The results in `folds` can be used as inputs into the `index` argument of the `trainControl` function.